\chapter{Introduction}
This chapter includes co-authored material previously published in EuroMPI~\cite{EuroMPI} and Parallel Computing~\cite{ParCo}. These papers were the result of a collaboration with Aur\`{e}le Mah\'{e}o, Sameer Shende, Allen Malony, Hari Subramoni, Amit Ruhela, and Dhabaleswar (DK) Panda. I wrote the entire introduction section. My co-authors contributed by suggesting edits to improve the content. 
\par The Message Passing Interface~\cite{MPI_3_1} remains the dominant programming model employed in scalable high-performance computing (HPC) applications. As a result, MPI performance engineering is worthwhile and plays a crucial role in improving the scalability of these applications. Traditionally, the first step in MPI performance engineering has been to profile the code to identify MPI operations that occupy a significant portion of runtime. MPI profiling is typically performed through the PMPI interface~\cite{PMPI}, wherein the performance profiler intercepts the MPI operation and performs the necessary timing operations within a wrapper function with the same name as the MPI operation. It then calls the corresponding name-shifted PMPI interface for this MPI operation. This technique generates accurate profiles without necessitating application code changes. The TAU Performance System$^{\tiny{\textregistered}}$~\cite{Shende:2006:TPP:1125980.1125982} is a popular tool that offers the user a comprehensive list of features to profile MPI applications through the PMPI profiling interface. PMPI profiling using TAU is performed transparently without modifying the application using runtime pre-loading of shared objects. \par
Although it plays a pivotal role in MPI performance engineering, using PMPI alone has some limitations:
\begin{itemize}
\item The profiler can only collect timing and message size data --- it does not have access to MPI internal performance metrics that can help detect and explain performance issues.
\item Profiling through the PMPI interface is mostly passive --- it provides limited scope for interaction between the profiler and the MPI implementation.
\end{itemize}
\par Performance characteristics of underlying hardware are constantly evolving as HPC moves toward increasingly heterogeneous platforms. MPI implementations available today~\cite{MVAPICH2,OpenMPI,MPICH,pjn2008} are complex software involving many modular components and offer the user a number of tunable environment variables that can affect performance. In such a setting, performance variations and scalability limitations can come from several sources. Detecting these performance limitations requires a more intimate understanding of MPI internals that cannot be elicited from the PMPI interface alone. \par
Tuning MPI library behavior through modification of environment variables presents a daunting challenge to the user --- among the rich variety of variables on offer, the user may not be aware of the right setting to modify, or the optimal value for a setting. Further, tuning through MPI environment variables has a notable limitation --- there is no way to fine-tune the MPI library at runtime. Runtime introspection and tuning are especially valuable to applications that display different behavior between phases, and one static setting of MPI parameters may not be optimal for the course of an entire run. In addition to this, each process may behave differently, and thus have a different optimal value for a given setting. \par
These complexities motivate the need for a performance measurement system such as TAU to play a more active role in the performance debugging and tuning process. With the introduction of the \textit{MPI Tools Information Interface (MPI\_T)} in the MPI 3.0 standard, there is now a standardized mechanism through which MPI libraries and external performance tuning software can share information. 
\par This document describes a software engineering infrastructure that enables an MPI implementation to interact with performance tuning software for the purpose of runtime introspection and tuning through the MPI\_T interface. Specifically, I describe the design of an infrastructure to enable performance monitoring, runtime introspection, performance tuning, and recommendation generation of MPI applications using TAU, and an infrastructure designed for runtime introspection of MPI using Caliper~\cite{CALIPER}.

\section {Thesis Outline}
While both the tools described in this document --- TAU and Caliper offer performance measurement and analysis capabilities, they differ in terms of the data model used to store performance information, support for automatic program instrumentation, and the level of integration with other performance profiling and tracing toolkits. In terms of support for different performance engineering related tasks, TAU offers the user a much broader and \textit{complete} set of features. Caliper is designed as a framework that relies on source-code annotation for data collection, and plugin based \textit{services} that can be combined to provide custom features to the user.

\subsection {Background and Related Work}
In \textbf{Chapter 2} of the thesis, I discuss background material and related work in the areas of interfaces for runtime introspection, autotuning of MPI runtimes, and tools that generate performance recommendations. I briefly introduce the  software that are a part of this study.

\subsection {Design of MPI\_T support in TAU}
In \textbf{Chapter 3}, I describe the design of the MPI\_T support in TAU for performance introspection, monitoring, autotuning, and recommendation generation. Although the TAU MPI\_T support is compliant with the MPI standard, it was designed in close collaboration with the MVAPICH2~\cite{MVAPICH2} MPI implementation. Thus, this chapter describes the MVAPICH2-specific use cases that were used to motivate the design of the infrastructure. This chapter ends with a brief note on some of the challenges faced while implementing the design.

\subsection {Design of MPI\_T support in Caliper}
In \textbf{Chapter 4}, I describe the design of the MPI\_T support in Caliper for performance introspection. This work was performed at the Center for Applied Scientific Computing, Lawrence Livermore National Laboratory, during the summer of 2017. Caliper support was designed specifically for use with OpenMPI~\cite{OpenMPI}. This shall conclude with a description of the unique design challenges encountered while working with OpenMPI.

\subsection {Discussion}
In \textbf{Chapter 5}, I present a discussion focusing on the differences between design and implementation of the MPI\_T support in TAU and Caliper. Specifically, I focus on describing the differences in performance introspection alone, as Caliper at the moment does not have a GUI-based performance monitoring or plugin-like tuning support for MPI\_T. This discussion shall highlight the advantages and pitfalls of one design methodology over the other, and suggest areas for future work. 

\subsection {Conclusion and Future Work}
In the concluding chapter, I shall present some directions for future work with regard to the MPI\_T support in TAU. Specifically, I shall discuss my ongoing research efforts in exploring the potential benefits of enabling extremely fine-grained tuning of MPI point-to-point rendezvous protocols for non-blocking communication at runtime.

\section {Coauthored Material}
This thesis includes previously published co-authored material. 
\par Chapter 2 and Chapter 3 include co-authored material previously published in the Proceedings of the 24th European MPI Users' Group Meeting (EuroMPI/USA 2017)~\cite{EuroMPI}, and the Journal of Parallel Computing~\cite{ParCo}.
