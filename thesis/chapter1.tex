\chapter{Introduction}

\section {Introduction}

The Message Passing Interface~\cite{MPI_3_1} remains the dominant programming model employed in scalable high-performance computing (HPC) applications. As a result, MPI performance engineering is worthwhile and plays a crucial role in improving the scalability of these applications. Traditionally, the first step in MPI performance engineering has been to profile the code to identify MPI operations that occupy a significant portion of runtime. MPI profiling is typically performed through the PMPI~\cite{PMPI} interface, wherein the performance profiler intercepts the MPI operation and performs the necessary timing operations within a wrapper function with the same name as the MPI operation. It then calls the corresponding name-shifted PMPI interface for this MPI operation. This technique generates accurate profiles without necessitating application code changes. The TAU Performance System$^{\textregistered}$~\cite{Shende:2006:TPP:1125980.1125982} is a popular tool that offers the user a comprehensive list of features to profile MPI applications through the PMPI profiling interface. PMPI profiling using TAU is performed transparently without modifying the application using runtime pre-loading of shared objects. \par
Although it plays a pivotal role in MPI performance engineering, using PMPI alone has some limitations:
\begin{itemize}
\item The profiler can only collect timing and message size data --- it does not have access to MPI internal performance metrics that can help detect and explain performance issues
\item Profiling through the PMPI interface is mostly passive --- it provides limited scope for interaction between the profiler and the MPI implementation
\end{itemize}
\par Performance characteristics of underlying hardware are constantly evolving as HPC moves toward increasingly heterogeneous platforms. MPI implementations available today~\cite{MVAPICH2,OpenMPI,MPICH,pjn2008} are complex software involving many modular components and offer the user a number of tunable environment variables that can affect performance. In such a setting, performance variations and scalability limitations can come from several sources. Detecting these performance limitations requires a more intimate understanding of MPI internals that cannot be elicited from the PMPI interface alone. \par
Tuning MPI library behavior through modification of environment variables presents a daunting challenge to the user --- among the rich variety of variables on offer, the user may not be aware of the right setting to modify, or the optimal value for a setting. Further, tuning through MPI environment variables has a notable limitation --- there is no way to fine-tune the MPI library at runtime. Runtime introspection and tuning are especially valuable to applications that display different behavior between phases, and one static setting of MPI parameters may not be optimal for the course of an entire run. In addition to this, each process may behave differently, and thus have a different optimal value for a given setting. \par
These complexities motivate the need for a performance measurement system such as TAU to play a more active role in the performance debugging and tuning process. With the introduction of the \textit{MPI Tools Information Interface (MPI\_T)} in the MPI 3.0 standard, there is now a standardized mechanism through which MPI libraries and external performance tuning software can share information. \par
This paper describes a software engineering infrastructure that enables an MPI implementation to interact with performance tuning software for the purpose of runtime introspection and tuning through the MPI\_T interface. We implement such an infrastructure with the integration of MVAPICH2~\cite{MVAPICH2} and TAU~\cite{Shende:2006:TPP:1125980.1125982}. We use a combination of production (AmberMD~\cite{AmberMD}), and synthetic applications (MiniAMR~\cite{Mantevo} and 3DStencil) as case studies to demonstrate the effectiveness of our design.

\section {Thesis Outline}

This paper makes the following contributions:
\begin{itemize}
	\item Enhance MPI\_T support in MVAPICH2 by developing a richer class of MPI\_T performance and control variables;
	\item Enable performance introspection and online monitoring through tight integration between MVAPICH2, TAU, and BEACON;
	\item Perform runtime autotuning through MPI\_T by developing plugin extensions for TAU; and 
	\item Generate performance recommendations through plugin extensions for TAU.
\end{itemize}
